{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import rdChemReactions, AllChem, Draw, PandasTools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mols = pd.read_pickle(\"../data/transformed/df_mols_ready.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mols['bbs_smi'] = df_mols['A_smi'] + '.' + df_mols['C_smi'] + '.' + df_mols['C_smi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = 0  # assuming pad token is 0\n",
    "\n",
    "def encode(smiles, stoi, max_len=128):\n",
    "    # returns list of token IDs with <s> at start, </s> at end, padded\n",
    "    ids = [stoi[\"<s>\"]] + [stoi[ch] for ch in smiles] + [stoi[\"</s>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "def decode(ids, itos):\n",
    "    # returns string ignoring special tokens\n",
    "    chars = [itos[i] for i in ids if itos[i] not in (\"<pad>\", \"<s>\", \"</s>\")]\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(smi, stoi, max_len=128):\n",
    "    full_ids = encode(smi, stoi, max_len)   # <s> ... </s> + pad\n",
    "    tgt_in = full_ids[:-1]                  # decoder input (drop final </s>)\n",
    "    tgt_out = full_ids[1:]                  # loss target (drop initial <s>)\n",
    "    return tgt_in, tgt_out\n",
    "\n",
    "# Apply to dataframe\n",
    "df_mols['tgt_in'], df_mols['tgt_out'] = zip(*df_mols['bbs_smi'].apply(lambda x: encode_target(x, stoi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_show_special(ids, itos):\n",
    "    chars = [itos[i] for i in ids]  # do NOT filter\n",
    "    return \"\".join(chars)\n",
    "\n",
    "print(\"Decoder input with specials:\", decode_show_special(sample_in, itos))\n",
    "print(\"Target for loss with specials:\", decode_show_special(sample_out, itos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDataset(Dataset):\n",
    "    def __init__(self, df, stoi, max_len=128):\n",
    "        self.src = [encode(s, stoi, max_len) for s in df['mol_smi']]\n",
    "        self.tgt_in = df['tgt_in'].tolist()\n",
    "        self.tgt_out = df['tgt_out'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt_in[idx]), torch.tensor(self.tgt_out[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a: Split dataframe into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_mols, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create datasets\n",
    "train_dataset = MolDataset(df_train, stoi)\n",
    "val_dataset   = MolDataset(df_val, stoi)\n",
    "\n",
    "# Step 4: Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab_size, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab_size, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "model = Seq2Seq(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # ignore padding in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 30\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "checkpoint_path = \"seq2seq_gru_bbs.pt\"\n",
    "\n",
    "# --- Resume from checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_in, tgt_out in loader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "            total_loss += loss.item() * src.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss 0.0354 | val loss 0.0351\n",
      "Epoch 01 | train loss 0.0352 | val loss 0.0425\n",
      "Epoch 02 | train loss 0.0339 | val loss 0.0429\n",
      "Epoch 03 | train loss 0.0346 | val loss 0.0388\n",
      "Epoch 04 | train loss 0.0323 | val loss 0.0411\n",
      "Epoch 05 | train loss 0.0329 | val loss 0.0408\n",
      "Epoch 06 | train loss 0.0322 | val loss 0.0411\n",
      "Epoch 07 | train loss 0.0310 | val loss 0.0464\n",
      "Epoch 08 | train loss 0.0311 | val loss 0.0402\n",
      "Epoch 09 | train loss 0.0277 | val loss 0.0408\n",
      "Epoch 10 | train loss 0.0272 | val loss 0.0409\n",
      "Epoch 11 | train loss 0.0356 | val loss 0.0444\n",
      "Epoch 12 | train loss 0.0301 | val loss 0.0432\n",
      "Epoch 13 | train loss 0.0281 | val loss 0.0425\n",
      "Epoch 14 | train loss 0.0259 | val loss 0.0374\n",
      "Epoch 15 | train loss 0.0258 | val loss 0.0394\n",
      "Epoch 16 | train loss 0.0252 | val loss 0.0431\n",
      "Epoch 17 | train loss 0.0273 | val loss 0.0407\n",
      "Epoch 18 | train loss 0.0270 | val loss 0.0405\n",
      "Epoch 19 | train loss 0.0259 | val loss 0.0409\n",
      "Epoch 20 | train loss 0.0244 | val loss 0.0400\n",
      "Epoch 21 | train loss 0.0258 | val loss 0.0417\n",
      "Epoch 22 | train loss 0.0247 | val loss 0.0411\n",
      "Epoch 23 | train loss 0.0243 | val loss 0.0410\n",
      "Epoch 24 | train loss 0.0233 | val loss 0.0410\n",
      "Epoch 25 | train loss 0.0222 | val loss 0.0394\n",
      "Epoch 26 | train loss 0.0243 | val loss 0.0408\n",
      "Epoch 27 | train loss 0.0241 | val loss 0.0433\n",
      "Epoch 28 | train loss 0.0219 | val loss 0.0404\n",
      "Epoch 29 | train loss 0.0199 | val loss 0.0400\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in train_loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * src.size(0)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # --- Compute validation loss\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # --- Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bbs(model, src_smiles, stoi, itos, max_len=128, device=device):\n",
    "    model.eval()\n",
    "    src_ids = torch.tensor([encode(src_smiles, stoi, max_len)]).to(device)\n",
    "    \n",
    "    # --- Encode once\n",
    "    _, h = model.enc(src_ids)\n",
    "    \n",
    "    # --- Decoder starts with <s>\n",
    "    tgt_ids = torch.tensor([[stoi[\"<s>\"]]], device=device)\n",
    "    pred_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, h = model.dec(tgt_ids, h)\n",
    "        next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "        if next_token.item() == stoi[\"</s>\"]:\n",
    "            break\n",
    "        pred_tokens.append(next_token.item())\n",
    "        tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "\n",
    "    pred_bbs = \"\".join([itos[i] for i in pred_tokens])\n",
    "    return pred_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@H](F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*]Cc1ccc(OC(F)(F)F)cc1.[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]Cc1ccc(OC(F)(F)C(F)F)cc1.[*]Cc1ccc(OC(F)(F)C(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CC(F)(F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*]Cc1ccc(OC(F)(F)F)cc1.[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1C(F)(F)F.[*]Cc1ccc(OC(F)(F)F)cc1.[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CCC(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*]Cc1ccc(OC(F)(F)F)cc1.[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]Cc1ccc(OC(F)(F)C(F)F)cc1.[*]Cc1ccc(OC(F)(F)C(F)F)cc1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    mol_smi = df_mols['mol_smi'].iloc[i]\n",
    "    true_bbs = df_mols['bbs_smi'].iloc[i]\n",
    "    pred_bbs = predict_bbs(model, mol_smi, stoi, itos)\n",
    "    print(f\"MOL  : {mol_smi}\")\n",
    "    print(f\"TRUE : {true_bbs}\")\n",
    "    print(f\"PRED : {pred_bbs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
