{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import rdChemReactions, AllChem, Draw, PandasTools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mols = pd.read_pickle(\"../data/transformed/df_mols_ready.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mols['bbs_smi'] = df_mols['A_smi'] + '.' + df_mols['B_smi'] + '.' + df_mols['C_smi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode/decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<pad>\", \"<s>\", \"</s>\"]\n",
    "\n",
    "all_smiles = df_mols['mol_smi'].tolist() + df_mols['bbs_smi'].tolist()\n",
    "chars = set(\"\".join(all_smiles))\n",
    "itos = special_tokens + sorted(list(chars))   # index-to-string\n",
    "stoi = {ch: i for i, ch in enumerate(itos)}   # string-to-index\n",
    "pad_id = stoi[\"<pad>\"]\n",
    "\n",
    "def encode(smiles, stoi, max_len=128):\n",
    "    # returns list of token IDs with <s> at start, </s> at end, padded\n",
    "    ids = [stoi[\"<s>\"]] + [stoi[ch] for ch in smiles] + [stoi[\"</s>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "def decode(ids, itos):\n",
    "    # returns string ignoring special tokens\n",
    "    chars = [itos[i] for i in ids if itos[i] not in (\"<pad>\", \"<s>\", \"</s>\")]\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(smi, stoi, max_len=128):\n",
    "    full_ids = encode(smi, stoi, max_len)   # <s> ... </s> + pad\n",
    "    tgt_in = full_ids[:-1]                  # decoder input (drop final </s>)\n",
    "    tgt_out = full_ids[1:]                  # loss target (drop initial <s>)\n",
    "    return tgt_in, tgt_out\n",
    "\n",
    "# Apply to dataframe\n",
    "df_mols['tgt_in'], df_mols['tgt_out'] = zip(*df_mols['bbs_smi'].apply(lambda x: encode_target(x, stoi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_show_special(ids, itos):\n",
    "    chars = [itos[i] for i in ids]  # do NOT filter\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolDataset(Dataset):\n",
    "    def __init__(self, df, stoi, max_len=128):\n",
    "        self.src = [encode(s, stoi, max_len) for s in df['mol_smi']]\n",
    "        self.tgt_in = df['tgt_in'].tolist()\n",
    "        self.tgt_out = df['tgt_out'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt_in[idx]), torch.tensor(self.tgt_out[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2a: Split dataframe into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_mols, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Create datasets\n",
    "train_dataset = MolDataset(df_train, stoi)\n",
    "val_dataset   = MolDataset(df_val, stoi)\n",
    "\n",
    "# Step 4: Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab_size, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab_size, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "model = Seq2Seq(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # ignore padding in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_in, tgt_out in loader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "            total_loss += loss.item() * src.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 60\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "checkpoint_path = \"seq2seq_gru_bbs.pt\"\n",
    "\n",
    "# --- Resume from checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 | train loss 0.0525 | val loss 0.1054\n",
      "Epoch 61 | train loss 0.0513 | val loss 0.1053\n",
      "Epoch 62 | train loss 0.0530 | val loss 0.1072\n",
      "Epoch 63 | train loss 0.0534 | val loss 0.1018\n",
      "Epoch 64 | train loss 0.0513 | val loss 0.1011\n",
      "Epoch 65 | train loss 0.0512 | val loss 0.1029\n",
      "Epoch 66 | train loss 0.0530 | val loss 0.1034\n",
      "Epoch 67 | train loss 0.0499 | val loss 0.1002\n",
      "Epoch 68 | train loss 0.0509 | val loss 0.1022\n",
      "Epoch 69 | train loss 0.0483 | val loss 0.1057\n",
      "Epoch 70 | train loss 0.0488 | val loss 0.0989\n",
      "Epoch 71 | train loss 0.0482 | val loss 0.1015\n",
      "Epoch 72 | train loss 0.0471 | val loss 0.1023\n",
      "Epoch 73 | train loss 0.0474 | val loss 0.1005\n",
      "Epoch 74 | train loss 0.0493 | val loss 0.1013\n",
      "Epoch 75 | train loss 0.0459 | val loss 0.1015\n",
      "Epoch 76 | train loss 0.0472 | val loss 0.1070\n",
      "Epoch 77 | train loss 0.0474 | val loss 0.1009\n",
      "Epoch 78 | train loss 0.0457 | val loss 0.1079\n",
      "Epoch 79 | train loss 0.0472 | val loss 0.1066\n",
      "Epoch 80 | train loss 0.0467 | val loss 0.1054\n",
      "Epoch 81 | train loss 0.0468 | val loss 0.1047\n",
      "Epoch 82 | train loss 0.0464 | val loss 0.1073\n",
      "Epoch 83 | train loss 0.0457 | val loss 0.0996\n",
      "Epoch 84 | train loss 0.0454 | val loss 0.1002\n",
      "Epoch 85 | train loss 0.0444 | val loss 0.1011\n",
      "Epoch 86 | train loss 0.0436 | val loss 0.1000\n",
      "Epoch 87 | train loss 0.0443 | val loss 0.1034\n",
      "Epoch 88 | train loss 0.0440 | val loss 0.1031\n",
      "Epoch 89 | train loss 0.0436 | val loss 0.1016\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop\n",
    "for epoch in range(start_epoch, start_epoch+n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in train_loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * src.size(0)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # --- Compute validation loss\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # --- Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bbs(model, src_smiles, stoi, itos, max_len=128, device=device):\n",
    "    model.eval()\n",
    "    src_ids = torch.tensor([encode(src_smiles, stoi, max_len)]).to(device)\n",
    "    \n",
    "    # --- Encode once\n",
    "    _, h = model.enc(src_ids)\n",
    "    \n",
    "    # --- Decoder starts with <s>\n",
    "    tgt_ids = torch.tensor([[stoi[\"<s>\"]]], device=device)\n",
    "    pred_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, h = model.dec(tgt_ids, h)\n",
    "        next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "        if next_token.item() == stoi[\"</s>\"]:\n",
    "            break\n",
    "        pred_tokens.append(next_token.item())\n",
    "        tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "\n",
    "    pred_bbs = \"\".join([itos[i] for i in pred_tokens])\n",
    "    return pred_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@H](F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])C[C@]([H])(F)C(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CC(F)(F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CC(F)(F)C(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CCC(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    mol_smi = df_mols['mol_smi'].iloc[i]\n",
    "    true_bbs = df_mols['bbs_smi'].iloc[i]\n",
    "    pred_bbs = predict_bbs(model, mol_smi, stoi, itos)\n",
    "    print(f\"MOL  : {mol_smi}\")\n",
    "    print(f\"TRUE : {true_bbs}\")\n",
    "    print(f\"PRED : {pred_bbs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
