{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "import pickle\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import rdChemReactions, AllChem, Draw, PandasTools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval = pd.read_parquet(\"../data/transformed/trainval_dataset_augmented_encoded.parquet\")\n",
    "with open(\"../tokens/stoi.pkl\", \"rb\") as f:\n",
    "    stoi = pickle.load(f)\n",
    "\n",
    "with open(\"../tokens/itos.pkl\", \"rb\") as f:\n",
    "    itos = pickle.load(f)\n",
    "\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(smiles, stoi, max_len=128):\n",
    "    # returns list of token IDs with <s> at start, </s> at end, padded\n",
    "    ids = [stoi[\"<s>\"]] + [stoi[ch] for ch in smiles] + [stoi[\"</s>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "def decode(ids, itos):\n",
    "    # returns string ignoring special tokens\n",
    "    chars = [itos[i] for i in ids if itos[i] not in (\"<pad>\", \"<s>\", \"</s>\")]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def encode_target(smi, stoi, max_len=128):\n",
    "    full_ids = encode(smi, stoi, max_len)   # <s> ... </s> + pad\n",
    "    tgt_in = full_ids[:-1]                  # decoder input (drop final </s>)\n",
    "    tgt_out = full_ids[1:]                  # loss target (drop initial <s>)\n",
    "    return tgt_in, tgt_out\n",
    "    \n",
    "def decode_show_special(ids, itos):\n",
    "    chars = [itos[i] for i in ids]  # do NOT filter\n",
    "    return \"\".join(chars)\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    def __init__(self, df, stoi, max_len=128):\n",
    "        self.src = [encode(s, stoi, max_len) for s in df['mol_smi']]\n",
    "        self.tgt_in = df['tgt_in'].tolist()\n",
    "        self.tgt_out = df['tgt_out'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt_in[idx]), torch.tensor(self.tgt_out[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trainval\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MolDataset(df_train, stoi)\n",
    "val_dataset   = MolDataset(df_val, stoi)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab_size, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab_size, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = stoi[\"<pad>\"]\n",
    "vocab_size = len(stoi)\n",
    "model = Seq2Seq(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # ignore padding in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_in, tgt_out in loader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "            total_loss += loss.item() * src.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 20\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "checkpoint_path = \"../models/seq2seq_gru_bbs.pt\"\n",
    "\n",
    "# --- Resume from checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train loss 0.0372 | val loss 0.0369\n",
      "Epoch 21 | train loss 0.0373 | val loss 0.0377\n",
      "Epoch 22 | train loss 0.0399 | val loss 0.0370\n",
      "Epoch 23 | train loss 0.0368 | val loss 0.0363\n",
      "Epoch 24 | train loss 0.0370 | val loss 0.0360\n",
      "Epoch 25 | train loss 0.0350 | val loss 0.0364\n",
      "Epoch 26 | train loss 0.0352 | val loss 0.0350\n",
      "Epoch 27 | train loss 0.0366 | val loss 0.0386\n",
      "Epoch 28 | train loss 0.0347 | val loss 0.0354\n",
      "Epoch 29 | train loss 0.0337 | val loss 0.0330\n",
      "Epoch 30 | train loss 0.0363 | val loss 0.0363\n",
      "Epoch 31 | train loss 0.0352 | val loss 0.0348\n",
      "Epoch 32 | train loss 0.0339 | val loss 0.0380\n",
      "Epoch 33 | train loss 0.0334 | val loss 0.0339\n",
      "Epoch 34 | train loss 0.0320 | val loss 0.0318\n",
      "Epoch 35 | train loss 0.0320 | val loss 0.0355\n",
      "Epoch 36 | train loss 0.0331 | val loss 0.0318\n",
      "Epoch 37 | train loss 0.0326 | val loss 0.0324\n",
      "Epoch 38 | train loss 0.0318 | val loss 0.0323\n",
      "Epoch 39 | train loss 0.0316 | val loss 0.0314\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop\n",
    "for epoch in range(start_epoch, start_epoch+n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in train_loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * src.size(0)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # --- Compute validation loss\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # --- Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
