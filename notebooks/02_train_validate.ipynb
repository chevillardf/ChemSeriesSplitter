{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import rdChemReactions, AllChem, Draw, PandasTools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval = pd.read_parquet(\"../data/transformed/trainval_dataset_augmented_encoded.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train / validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_show_special(ids, itos):\n",
    "    chars = [itos[i] for i in ids]  # do NOT filter\n",
    "    return \"\".join(chars)\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "    def __init__(self, df, stoi, max_len=128):\n",
    "        self.src = [encode(s, stoi, max_len) for s in df['mol_smi']]\n",
    "        self.tgt_in = df['tgt_in'].tolist()\n",
    "        self.tgt_out = df['tgt_out'].tolist()\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt_in[idx]), torch.tensor(self.tgt_out[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split trainval\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MolDataset(df_train, stoi)\n",
    "val_dataset   = MolDataset(df_val, stoi)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab_size, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab_size, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(stoi)\n",
    "model = Seq2Seq(vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)  # ignore padding in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_in, tgt_out in loader:\n",
    "            src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "            logits = model(src, tgt_in)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "            total_loss += loss.item() * src.size(0)\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "checkpoint_path = \"../models/seq2seq_gru_bbs.pt\"\n",
    "\n",
    "# --- Resume from checkpoint if exists\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train loss 0.2706 | val loss 0.1378\n",
      "Epoch 01 | train loss 0.1252 | val loss 0.1084\n",
      "Epoch 02 | train loss 0.0992 | val loss 0.0894\n",
      "Epoch 03 | train loss 0.0823 | val loss 0.0752\n",
      "Epoch 04 | train loss 0.0715 | val loss 0.0705\n",
      "Epoch 05 | train loss 0.0657 | val loss 0.0635\n",
      "Epoch 06 | train loss 0.0611 | val loss 0.0597\n",
      "Epoch 07 | train loss 0.0576 | val loss 0.0571\n",
      "Epoch 08 | train loss 0.0548 | val loss 0.0529\n",
      "Epoch 09 | train loss 0.0523 | val loss 0.0519\n",
      "Epoch 10 | train loss 0.0495 | val loss 0.0461\n",
      "Epoch 11 | train loss 0.0467 | val loss 0.0464\n",
      "Epoch 12 | train loss 0.0463 | val loss 0.0461\n",
      "Epoch 13 | train loss 0.0433 | val loss 0.0428\n",
      "Epoch 14 | train loss 0.0439 | val loss 0.0417\n",
      "Epoch 15 | train loss 0.0414 | val loss 0.0407\n",
      "Epoch 16 | train loss 0.0411 | val loss 0.0409\n",
      "Epoch 17 | train loss 0.0398 | val loss 0.0393\n",
      "Epoch 18 | train loss 0.0401 | val loss 0.0441\n",
      "Epoch 19 | train loss 0.0390 | val loss 0.0370\n"
     ]
    }
   ],
   "source": [
    "# --- Training loop\n",
    "for epoch in range(start_epoch, start_epoch+n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in train_loader:\n",
    "        src, tgt_in, tgt_out = src.to(device), tgt_in.to(device), tgt_out.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item() * src.size(0)\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # --- Compute validation loss\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # --- Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} | val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': 90,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stoi': stoi,\n",
    "        'itos': itos\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
