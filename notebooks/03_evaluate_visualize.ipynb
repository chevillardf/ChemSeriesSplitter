{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from rdkit import Chem, DataStructs, RDLogger\n",
    "from rdkit.Chem import rdChemReactions, AllChem, Draw, PandasTools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mols = pd.read_pickle(\"../data/transformed/df_mols_ready.pkl\")\n",
    "df_test = df_mols.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits\n",
    "\n",
    "def encode(smiles, stoi, max_len=128):\n",
    "    # returns list of token IDs with <s> at start, </s> at end, padded\n",
    "    ids = [stoi[\"<s>\"]] + [stoi[ch] for ch in smiles] + [stoi[\"</s>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "def decode(ids, itos):\n",
    "    # returns string ignoring special tokens\n",
    "    chars = [itos[i] for i in ids if itos[i] not in (\"<pad>\", \"<s>\", \"</s>\")]\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = torch.load(\"../models/seq2seq_gru_bbs.pt\", map_location=device)\n",
    "\n",
    "# Re-create the model\n",
    "vocab_size = len(checkpoint['stoi'])\n",
    "model = Seq2Seq(vocab=vocab_size, d_model=384, n_layers=2, dropout=0.1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenizer dictionaries\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        out, h = self.rnn(x)\n",
    "        return out, h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, d_model, padding_idx=0)\n",
    "        self.rnn = nn.GRU(d_model, d_model, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    def forward(self, y_in, h):\n",
    "        y = self.emb(y_in)\n",
    "        out, h = self.rnn(y, h)\n",
    "        logits = self.proj(out)\n",
    "        return logits, h\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab, d_model=384, n_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(vocab, d_model, n_layers, dropout)\n",
    "        self.dec = Decoder(vocab, d_model, n_layers, dropout)\n",
    "    def forward(self, src, tgt_in):\n",
    "        _, h = self.enc(src)\n",
    "        logits, _ = self.dec(tgt_in, h)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = torch.load(\"../models/seq2seq_gru_bbs.pt\", map_location=device)\n",
    "\n",
    "# Re-create the model\n",
    "vocab_size = len(checkpoint['stoi'])\n",
    "model = Seq2Seq(vocab=vocab_size, d_model=384, n_layers=2, dropout=0.1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Tokenizer dictionaries\n",
    "stoi = checkpoint['stoi']\n",
    "itos = checkpoint['itos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bbs(model, src_smiles, stoi, itos, max_len=128, device=device):\n",
    "    model.eval()\n",
    "    src_ids = torch.tensor([encode(src_smiles, stoi, max_len)]).to(device)\n",
    "    \n",
    "    # --- Encode once\n",
    "    _, h = model.enc(src_ids)\n",
    "    \n",
    "    # --- Decoder starts with <s>\n",
    "    tgt_ids = torch.tensor([[stoi[\"<s>\"]]], device=device)\n",
    "    pred_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits, h = model.dec(tgt_ids, h)\n",
    "        next_token = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "        if next_token.item() == stoi[\"</s>\"]:\n",
    "            break\n",
    "        pred_tokens.append(next_token.item())\n",
    "        tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "\n",
    "    pred_bbs = \"\".join([itos[i] for i in pred_tokens])\n",
    "    return pred_bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@H](F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])C[C@]([H])(F)C(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CC(F)(F)C(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CC(F)(F)C(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CCC(=O)N1Cc1ccc(OC(F)(F)F)cc1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "PRED : [*]c1c(OC)cccc1OC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)cc1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@H](Cl)C(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])C[C@]([H])(Cl)C(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@H](F)C(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])C[C@]([H])(F)C(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CC(F)(F)C(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CC(F)(F)C(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CCC(=O)N1Cc1cccc(-c2csc(C)n2)c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CCC(=O)N1[*].[*]Cc1cccc(-c2csc(C)n2)c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1ccc(OC(F)(F)F)c(F)c1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1CCC(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])CCC(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n",
      "MOL  : COc1cccc(OC)c1[C@@H]1C[C@@H](O)C(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(OC)cccc1OC.[*][C@]1([H])C[C@@]([H])(O)C(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n",
      "MOL  : COc1cccc(F)c1[C@@H]1C[C@H](F)C(=O)N1Cc1ccc2oc3ccccc3c2c1\n",
      "TRUE : [*]c1c(F)cccc1OC.[*][C@]1([H])C[C@]([H])(F)C(=O)N1[*].[*]Cc1ccc2oc3ccccc3c2c1\n",
      "PRED : [*]c1c(OC)cccc1OCC.[*]C1CCCC(=O)N1[*].[*]Cc1cccc(OCC(F)F)c1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    mol_smi = df_test['mol_smi'].iloc[i]\n",
    "    true_bbs = df_test['bbs_smi'].iloc[i]\n",
    "    pred_bbs = predict_bbs(model, mol_smi, stoi, itos)\n",
    "    print(f\"MOL  : {mol_smi}\")\n",
    "    print(f\"TRUE : {true_bbs}\")\n",
    "    print(f\"PRED : {pred_bbs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles('[*][C@]1([H])C[C@]([H])(F)C(=O)N1[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles('[*]C1CCCC(=O)N1[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonize_smiles(smi):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return Chem.MolToSmiles(mol, canonical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(len(df_test)):\n",
    "    mol_smi = df_test['mol_smi'].iloc[i]\n",
    "    true_a = df_test['A_smi'].iloc[i]\n",
    "    pred_a = predict_bbs(model, mol_smi, stoi, itos).split('.')[0]  # first fragment as example\n",
    "    \n",
    "    true_can = canonize_smiles(true_a)\n",
    "    pred_can = canonize_smiles(pred_a)\n",
    "    \n",
    "    exact_match = true_can == pred_can\n",
    "    \n",
    "    results.append({\n",
    "        'mol_smi': mol_smi,\n",
    "        'true_a': true_a,\n",
    "        'pred_a': pred_a,\n",
    "        'true_can': true_can,\n",
    "        'pred_can': pred_can,\n",
    "        'exact_match': exact_match\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_image(smiles, size=(200,200)):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return Draw.MolToImage(mol, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_val.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(df_test)):\n",
    "    mol_smi = df_test['mol_smi'].iloc[i]\n",
    "    true_bbs = df_test['bbs_smi'].iloc[i]\n",
    "    pred_bbs = predict_bbs(model, mol_smi, stoi, itos)\n",
    "    \n",
    "    rows.append({\n",
    "        'Molecule': mol_to_image(mol_smi),\n",
    "        'True BBS': mol_to_image(true_bbs.split('.')[0]),  # optionally only first fragment\n",
    "        'Pred BBS': mol_to_image(pred_bbs.split('.')[0])\n",
    "    })\n",
    "\n",
    "df_vis = pd.DataFrame(rows)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for i, row in df_vis.iterrows():\n",
    "    display(row['Molecule'])\n",
    "    display(row['True BBS'])\n",
    "    display(row['Pred BBS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
